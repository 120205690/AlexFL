{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Axiomatize\\pytorch3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import math\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=torchvision.datasets.MNIST(root='data', download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "testset=torchvision.datasets.MNIST(root='data', download=True, train=False, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainlist=trainset.items()\n",
    "train_subset, val_subset = torch.utils.data.random_split(trainset, [50000, 10000], generator=torch.Generator().manual_seed(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_subset.dataset.data[train_subset.indices]\n",
    "y_train = train_subset.dataset.targets[train_subset.indices]\n",
    "\n",
    "x_valid = val_subset.dataset.data[val_subset.indices]\n",
    "y_valid = val_subset.dataset.targets[val_subset.indices]\n",
    "\n",
    "x_train = torch.stack([x.flatten() for x in x_train])/255\n",
    "x_valid = torch.stack([x.flatten() for x in x_valid])/255\n",
    "\n",
    "x_test = torch.stack ([x.flatten() for x, _ in testset])\n",
    "y_test = torch.tensor ([y for _, y in testset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "torch.float32\n",
      "tensor(1.)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(x_train.max())\n",
    "print(x_train.dtype)\n",
    "print(x_valid.max())\n",
    "# print(x_train[5])\n",
    "print(x_test.dtype)\n",
    "# print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the dataset size\n",
    "x_train.shape, y_train.shape , x_valid.shape, y_valid.shape, x_test.shape, y_test.shape                                                                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_shuffle_labels(y_data, seed, amount):\n",
    "    y_data=pd.DataFrame(y_data,columns=[\"labels\"])\n",
    "    y_data[\"i\"]=np.arange(len(y_data))\n",
    "    label_dict = dict()\n",
    "    for i in range(10):\n",
    "        var_name=\"label\" + str(i)\n",
    "        label_info=y_data[y_data[\"labels\"]==i]\n",
    "        np.random.seed(seed)\n",
    "        label_info=np.random.permutation(label_info)\n",
    "        label_info=label_info[0:amount]\n",
    "        label_info=pd.DataFrame(label_info, columns=[\"labels\",\"i\"])\n",
    "        label_dict.update({var_name: label_info })\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iid_subsamples_indices(label_dict, number_of_samples, amount):\n",
    "    sample_dict= dict()\n",
    "    batch_size=int(math.floor(amount/number_of_samples))\n",
    "    for i in range(number_of_samples):\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        dumb=pd.DataFrame()\n",
    "        for j in range(10):\n",
    "            label_name=str(\"label\")+str(j)\n",
    "            a=label_dict[label_name][i*batch_size:(i+1)*batch_size]\n",
    "            dumb=pd.concat([dumb,a], axis=0)\n",
    "        dumb.reset_index(drop=True, inplace=True)    \n",
    "        sample_dict.update({sample_name: dumb}) \n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iid_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()\n",
    "    \n",
    "    for i in range(len(sample_dict)):  ### len(sample_dict)= number of samples\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        \n",
    "        indices=np.sort(np.array(sample_dict[sample_name][\"i\"]))\n",
    "        \n",
    "        x_info= x_data[indices,:]\n",
    "        x_data_dict.update({xname : x_info})\n",
    "        \n",
    "        y_info= y_data[indices]\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(784,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # print(x.type())\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "train"
    ]
   },
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    olddata=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            # print(\"Non-zero\", torch.count_nonzero(data))\n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "            \n",
    "            olddate=data\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_optimizer_criterion_dict(number_of_samples):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn()\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_weights(model_dict, number_of_samples):\n",
    "   \n",
    "    fc1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape)\n",
    "    fc1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc1.bias.shape)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc2.weight.shape)\n",
    "    fc2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc2.bias.shape)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc3.weight.shape)\n",
    "    fc3_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc3.bias.shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "    \n",
    "        for i in range(number_of_samples):\n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/number_of_samples\n",
    "        fc1_mean_bias = fc1_mean_bias/ number_of_samples\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/number_of_samples\n",
    "        fc2_mean_bias = fc2_mean_bias/ number_of_samples\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/number_of_samples\n",
    "        fc3_mean_bias = fc3_mean_bias/ number_of_samples\n",
    "    \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples):\n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights(model_dict, number_of_samples=number_of_samples)\n",
    "    with torch.no_grad():\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_local_and_merged_model_performance(number_of_samples):\n",
    "    accuracy_table=pd.DataFrame(data=np.zeros((number_of_samples,3)), columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n",
    "    for i in range (number_of_samples):\n",
    "    \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        individual_loss, individual_accuracy = validation(model, test_dl, criterion)\n",
    "        main_loss, main_accuracy =validation(main_model, test_dl, main_criterion )\n",
    "    \n",
    "        accuracy_table.loc[i, \"sample\"]=\"sample \"+str(i)\n",
    "        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n",
    "        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n",
    "\n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_samples):\n",
    "\n",
    "            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone() \n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train_end_node_process(number_of_samples):\n",
    "    for i in range (number_of_samples): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         valid_ds = TensorDataset(x_valid_dict[name_of_x_valid_sets[i]], y_valid_dict[name_of_y_valid_sets[i]])\n",
    "#         valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "        \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        print(\"Subset\" ,i)\n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "#             valid_loss, valid_accuracy = validation(model, valid_dl, criterion)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "    \n",
    "            print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_train_end_node_process_without_print(number_of_samples):\n",
    "    for i in range (number_of_samples): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train_end_node_process_print_some(number_of_samples, print_amount):\n",
    "    for i in range (number_of_samples): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        if i<print_amount:\n",
    "            print(\"Subset\" ,i)\n",
    "            \n",
    "        for epoch in range(numEpoch):\n",
    "        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "            \n",
    "            if i<print_amount:        \n",
    "                print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Axiomatize\\AppData\\Local\\Temp\\ipykernel_11008\\1350096759.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train, y_train, x_valid, y_valid,x_test, y_test = map(torch.tensor, (x_train, y_train, x_valid, y_valid, x_test, y_test))\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid,x_test, y_test = map(torch.tensor, (x_train, y_train, x_valid, y_valid, x_test, y_test))\n",
    "number_of_samples=100\n",
    "learning_rate = 0.01\n",
    "numEpoch = 10\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "\n",
    "train_amount=4500\n",
    "valid_amount=900\n",
    "test_amount=900\n",
    "print_amount=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_model = Net2nn()\n",
    "centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# centralized_optimizer = torch.optim.Adam(centralized_model.parameters())\n",
    "\n",
    "centralized_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(testset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Centralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Centralized Model ------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 | train accuracy:  0.8774 | test accuracy:  0.9360\n",
      "epoch:   2 | train accuracy:  0.9581 | test accuracy:  0.9688\n",
      "epoch:   3 | train accuracy:  0.9717 | test accuracy:  0.9673\n",
      "epoch:   4 | train accuracy:  0.9785 | test accuracy:  0.9724\n",
      "epoch:   5 | train accuracy:  0.9838 | test accuracy:  0.9776\n",
      "epoch:   6 | train accuracy:  0.9866 | test accuracy:  0.9772\n",
      "epoch:   7 | train accuracy:  0.9900 | test accuracy:  0.9786\n",
      "epoch:   8 | train accuracy:  0.9921 | test accuracy:  0.9765\n",
      "epoch:   9 | train accuracy:  0.9941 | test accuracy:  0.9787\n",
      "epoch:  10 | train accuracy:  0.9957 | test accuracy:  0.9779\n",
      "------ Training finished ------\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Centralized Model ------\")\n",
    "for epoch in range(numEpoch):\n",
    "    central_train_loss, central_train_accuracy = train(centralized_model, train_dl, centralized_criterion, centralized_optimizer)\n",
    "    central_test_loss, central_test_accuracy = validation(centralized_model, test_dl, centralized_criterion)\n",
    "    \n",
    "    print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "\n",
    "print(\"------ Training finished ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_train=split_and_shuffle_labels(y_data=y_train, seed=1, amount=train_amount) \n",
    "sample_dict_train=get_iid_subsamples_indices(label_dict=label_dict_train, number_of_samples=number_of_samples, amount=train_amount)\n",
    "x_train_dict, y_train_dict = create_iid_subsamples(sample_dict=sample_dict_train, x_data=x_train, y_data=y_train, x_name=\"x_train\", y_name=\"y_train\")\n",
    "\n",
    "\n",
    "label_dict_valid = split_and_shuffle_labels(y_data=y_valid, seed=1, amount=train_amount) \n",
    "sample_dict_valid = get_iid_subsamples_indices(label_dict=label_dict_valid, number_of_samples=number_of_samples, amount=valid_amount)\n",
    "x_valid_dict, y_valid_dict = create_iid_subsamples(sample_dict=sample_dict_valid, x_data=x_valid, y_data=y_valid, x_name=\"x_valid\", y_name=\"y_valid\")\n",
    "\n",
    "label_dict_test = split_and_shuffle_labels(y_data=y_test, seed=1, amount=test_amount) \n",
    "sample_dict_test = get_iid_subsamples_indices(label_dict=label_dict_test, number_of_samples=number_of_samples, amount=test_amount)\n",
    "x_test_dict, y_test_dict = create_iid_subsamples(sample_dict=sample_dict_test, x_data=x_test, y_data=y_test, x_name=\"x_test\", y_name=\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([450, 784]) torch.Size([450])\n",
      "torch.Size([90, 784]) torch.Size([90])\n",
      "torch.Size([90, 784]) torch.Size([90])\n",
      "tensor(1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train_dict[\"x_train1\"].shape, y_train_dict[\"y_train1\"].shape)\n",
    "print(x_valid_dict[\"x_valid1\"].shape, y_valid_dict[\"y_valid1\"].shape) \n",
    "print(x_test_dict[\"x_test1\"].shape, y_test_dict[\"y_test1\"].shape)\n",
    "\n",
    "num_index = np.random.randint(test_amount/number_of_samples*10)\n",
    "pyplot.imshow(x_test_dict[\"x_test0\"][num_index].reshape((28,28)), cmap=\"gray\")\n",
    "print(y_test_dict[\"y_test0\"][num_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = Net2nn()\n",
    "main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_x_train_sets=list(x_train_dict.keys())\n",
    "name_of_y_train_sets=list(y_train_dict.keys())\n",
    "name_of_x_valid_sets=list(x_valid_dict.keys())\n",
    "name_of_y_valid_sets=list(y_valid_dict.keys())\n",
    "name_of_x_test_sets=list(x_test_dict.keys())\n",
    "name_of_y_test_sets=list(y_test_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())\n",
    "\n",
    "# print(name_of_x_train_sets)\n",
    "# print(name_of_y_train_sets)\n",
    "# print(name_of_x_valid_sets)\n",
    "# print(name_of_y_valid_sets)\n",
    "# print(name_of_x_test_sets)\n",
    "# print(name_of_y_test_sets)\n",
    "# print(\"\\n ------------\")\n",
    "# print(name_of_models)\n",
    "# print(name_of_optimizers)\n",
    "# print(name_of_criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.6699e-02, -3.6196e-02, -1.7823e-02, -1.8314e-05, -4.5419e-02]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.0317,  0.0699,  0.0150,  0.0292,  0.0013]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.6699e-02, -3.6196e-02, -1.7823e-02, -1.8314e-05, -4.5419e-02]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 3.6699e-02, -3.6196e-02, -1.7823e-02, -1.8314e-05, -4.5419e-02]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 0\n",
      "epoch:   1 | train accuracy: 0.11556 | test accuracy: 0.15556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   2 | train accuracy: 0.32000 | test accuracy: 0.37778\n",
      "epoch:   3 | train accuracy: 0.44667 | test accuracy: 0.41111\n",
      "epoch:   4 | train accuracy: 0.56667 | test accuracy: 0.68889\n",
      "epoch:   5 | train accuracy: 0.66889 | test accuracy: 0.72222\n",
      "epoch:   6 | train accuracy: 0.72444 | test accuracy: 0.81111\n",
      "epoch:   7 | train accuracy: 0.76000 | test accuracy: 0.80000\n",
      "epoch:   8 | train accuracy: 0.75111 | test accuracy: 0.84444\n",
      "epoch:   9 | train accuracy: 0.82889 | test accuracy: 0.70000\n",
      "epoch:  10 | train accuracy: 0.85111 | test accuracy: 0.84444\n",
      "Subset 1\n",
      "epoch:   1 | train accuracy: 0.11778 | test accuracy: 0.14444\n",
      "epoch:   2 | train accuracy: 0.27111 | test accuracy: 0.36667\n",
      "epoch:   3 | train accuracy: 0.56889 | test accuracy: 0.70000\n",
      "epoch:   4 | train accuracy: 0.63333 | test accuracy: 0.62222\n",
      "epoch:   5 | train accuracy: 0.57333 | test accuracy: 0.67778\n",
      "epoch:   6 | train accuracy: 0.69556 | test accuracy: 0.74444\n",
      "epoch:   7 | train accuracy: 0.75556 | test accuracy: 0.80000\n",
      "epoch:   8 | train accuracy: 0.79333 | test accuracy: 0.77778\n",
      "epoch:   9 | train accuracy: 0.79333 | test accuracy: 0.72222\n",
      "epoch:  10 | train accuracy: 0.81778 | test accuracy: 0.77778\n",
      "Subset 2\n",
      "epoch:   1 | train accuracy: 0.11778 | test accuracy: 0.20000\n",
      "epoch:   2 | train accuracy: 0.21333 | test accuracy: 0.24444\n",
      "epoch:   3 | train accuracy: 0.30000 | test accuracy: 0.33333\n",
      "epoch:   4 | train accuracy: 0.54889 | test accuracy: 0.55556\n",
      "epoch:   5 | train accuracy: 0.50889 | test accuracy: 0.54444\n",
      "epoch:   6 | train accuracy: 0.67111 | test accuracy: 0.66667\n",
      "epoch:   7 | train accuracy: 0.76444 | test accuracy: 0.70000\n",
      "epoch:   8 | train accuracy: 0.80000 | test accuracy: 0.68889\n",
      "epoch:   9 | train accuracy: 0.79333 | test accuracy: 0.70000\n",
      "epoch:  10 | train accuracy: 0.82667 | test accuracy: 0.75556\n"
     ]
    }
   ],
   "source": [
    "# start_train_end_node_process()\n",
    "start_train_end_node_process_print_some(number_of_samples, print_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.6699e-02, -3.6196e-02, -1.7823e-02, -1.8314e-05, -4.5419e-02],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([ 0.0323, -0.0339, -0.0066,  0.0192, -0.0501], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#As you can see, wieghts of local models are updated after training process\n",
    "print(main_model.fc2.weight[0,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_acc_table=compare_local_and_merged_model_performance(number_of_samples=number_of_samples)\n",
    "before_test_loss, before_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "\n",
    "main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples) \n",
    "\n",
    "after_acc_table=compare_local_and_merged_model_performance(number_of_samples=number_of_samples)\n",
    "after_test_loss, after_test_accuracy = validation(main_model, test_dl, main_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated main model vs individual local models before FedAvg first iteration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>local_ind_model</th>\n",
       "      <th>merged_main_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample 0</td>\n",
       "      <td>0.8444</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample 1</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample 2</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>0.0889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample 3</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>0.1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample 4</td>\n",
       "      <td>0.7333</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample  local_ind_model  merged_main_model\n",
       "0  sample 0           0.8444             0.1000\n",
       "1  sample 1           0.7778             0.1000\n",
       "2  sample 2           0.7556             0.0889\n",
       "3  sample 3           0.7556             0.1111\n",
       "4  sample 4           0.7333             0.1000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Federated main model vs individual local models before FedAvg first iteration\")\n",
    "before_acc_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated main model vs individual local models after FedAvg first iteration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>local_ind_model</th>\n",
       "      <th>merged_main_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample 0</td>\n",
       "      <td>0.8444</td>\n",
       "      <td>0.8667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample 1</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.8444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample 2</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>0.7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample 3</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>0.8444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample 4</td>\n",
       "      <td>0.7333</td>\n",
       "      <td>0.8778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample  local_ind_model  merged_main_model\n",
       "0  sample 0           0.8444             0.8667\n",
       "1  sample 1           0.7778             0.8444\n",
       "2  sample 2           0.7556             0.7778\n",
       "3  sample 3           0.7556             0.8444\n",
       "4  sample 4           0.7333             0.8778"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Federated main model vs individual local models after FedAvg first iteration\")\n",
    "after_acc_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 1st iteration main model accuracy on all test data:  0.1026\n",
      "After 1st iteration main model accuracy on all test data:  0.8510\n",
      "Centralized model accuracy on all test data:  0.9779\n"
     ]
    }
   ],
   "source": [
    "print(\"Before 1st iteration main model accuracy on all test data: {:7.4f}\".format(before_test_accuracy))\n",
    "print(\"After 1st iteration main model accuracy on all test data: {:7.4f}\".format(after_test_accuracy))\n",
    "print(\"Centralized model accuracy on all test data: {:7.4f}\".format(central_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples)\n",
    "    start_train_end_node_process_without_print(number_of_samples)\n",
    "    main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples) \n",
    "    test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "    print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
